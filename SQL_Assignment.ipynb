{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YVamtO8t2zyS"
   },
   "source": [
    "# Installing Snowflake Connector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "d-HrevD2uJD9",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: snowflake-connector-python in /opt/conda/lib/python3.11/site-packages (3.1.1)\n",
      "Requirement already satisfied: asn1crypto<2.0.0,>0.24.0 in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python) (1.5.1)\n",
      "Requirement already satisfied: cffi<2.0.0,>=1.9 in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python) (1.15.1)\n",
      "Requirement already satisfied: cryptography<42.0.0,>=3.1.0 in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python) (41.0.1)\n",
      "Requirement already satisfied: oscrypto<2.0.0 in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python) (1.3.0)\n",
      "Requirement already satisfied: pyOpenSSL<24.0.0,>=16.2.0 in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python) (23.2.0)\n",
      "Requirement already satisfied: pycryptodomex!=3.5.0,<4.0.0,>=3.2 in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python) (3.18.0)\n",
      "Requirement already satisfied: pyjwt<3.0.0 in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python) (2.7.0)\n",
      "Requirement already satisfied: pytz in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python) (2023.3)\n",
      "Requirement already satisfied: requests<3.0.0 in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python) (2.31.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python) (23.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python) (2023.5.7)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.3 in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python) (4.6.3)\n",
      "Requirement already satisfied: filelock<4,>=3.5 in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python) (3.12.2)\n",
      "Requirement already satisfied: sortedcontainers>=2.4.0 in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python) (2.4.0)\n",
      "Requirement already satisfied: platformdirs<3.9.0,>=2.6.0 in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python) (3.8.0)\n",
      "Requirement already satisfied: tomlkit in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python) (0.12.1)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.11/site-packages (from cffi<2.0.0,>=1.9->snowflake-connector-python) (2.21)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade snowflake-connector-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1RJ47qXa3AbK"
   },
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "MrBluxVnuNNZ",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/snowflake/connector/options.py:103: UserWarning: You have an incompatible version of 'pyarrow' installed (12.0.0), please install a version that adheres to: 'pyarrow<10.1.0,>=10.0.1; extra == \"pandas\"'\n",
      "  warn_incompatible_dep(\n"
     ]
    }
   ],
   "source": [
    "import snowflake.connector\n",
    "import glob\n",
    "import sqlite3\n",
    "import os\n",
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fH5Jdzny3Xbl"
   },
   "source": [
    "# Connecting to Snowflake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "KVJpCxw3uSsV",
    "tags": []
   },
   "outputs": [],
   "source": [
    "conn = snowflake.connector.connect(\n",
    "    user='oakpongphon',\n",
    "    password='Oak053839212*',\n",
    "    account='DYB98379',\n",
    "    warehouse= 'my_first_warehouse',\n",
    "    database= 'testdb12',\n",
    "    schema= 'testschema12'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "32uVrlrU3lpA"
   },
   "source": [
    "# Creating new warehouse, database and schema inside Snowflake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "9dl13pwbuaK0",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<snowflake.connector.cursor.SnowflakeCursor at 0xffff5b462490>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cs = conn.cursor()\n",
    "cs.execute(\"CREATE WAREHOUSE IF NOT EXISTS my_first_warehouse\")\n",
    "cs.execute(\"CREATE DATABASE IF NOT EXISTS testdb12\")\n",
    "cs.execute(\"CREATE SCHEMA IF NOT EXISTS testschema12\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nkek6WUo3sVp"
   },
   "source": [
    "# Creating a blank table for Purchase Order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "hB7DHHYHujGg",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<snowflake.connector.cursor.SnowflakeCursor at 0xffff5b462490>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cs.execute(\n",
    "    \"CREATE OR REPLACE TABLE po_table(\" \\\n",
    "    \"PurchaseOrderID INT, SupplierID INT,\" \\\n",
    "    \"OrderDate DATE, DeliveryMethodID INT,\" \\\n",
    "    \"ContactPersonID INT, ExpectedDeliveryDate STRING,\" \\\n",
    "    \"SupplierReference STRING, IsOrderFinalized INT,\" \\\n",
    "    \"Comments FLOAT, InternalComments FLOAT,\" \\\n",
    "    \"LastEditedBy INT, LastEditedWhen STRING,\" \\\n",
    "    \"PurchaseOrderLineID INT, StockItemID INT,\" \\\n",
    "    \"OrderedOuters INT, Description STRING,\" \\\n",
    "    \"ReceivedOuters INT, PackageTypeID INT,\" \\\n",
    "    \"ExpectedUnitPricePerOuter FLOAT, LastReceiptDate STRING,\" \\\n",
    "    \"IsOrderLineFinalized INT, Right_LastEditedBy INT,\" \\\n",
    "    \"Right_LastEditedWhen STRING)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QQcfidx035Wf"
   },
   "source": [
    "# Using glob to iterate through and put all purchases files automatically, creating a stage and using copy into to ingest the data into the Purchase Order Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "0Ee9VU9QuHT-",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a stage named \"local_stage\"\n",
    "cs.execute(\"CREATE OR REPLACE STAGE local_stage\")\n",
    "\n",
    "# Your directory path containing the CSV files\n",
    "directory_path = 'Documents/UCSD/495_SQL/Assignment/Group_project/Case_Data/Monthly_PO_Data/'\n",
    "\n",
    "# Use glob or your custom logic to fetch all relevant CSV files\n",
    "csv_files = glob.glob(directory_path + '201[3-6]-*.csv')\n",
    "\n",
    "# Upload files to the stage\n",
    "for csv_file in csv_files:\n",
    "    with open(csv_file, 'rb') as f:\n",
    "        cs.execute(\"PUT file://{} @local_stage\".format(csv_file))\n",
    "\n",
    "# Use COPY INTO to move data from stage to the table\n",
    "for csv_file in csv_files:\n",
    "    file_name = os.path.basename(csv_file)\n",
    "    cs.execute(f\"\"\"\n",
    "        COPY INTO po_table\n",
    "        FROM @local_stage/{file_name}\n",
    "        FILE_FORMAT = (TYPE = 'CSV' SKIP_HEADER = 1 FIELD_OPTIONALLY_ENCLOSED_BY = '\"' NULL_IF=('NULL', '\\\\N'))\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yP78Ucub4int"
   },
   "source": [
    "# Adding a new column POAmount in the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "fE1C28Zz1xQ3",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<snowflake.connector.cursor.SnowflakeCursor at 0xffff5b462490>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cs.execute(\"\"\"\n",
    "    ALTER TABLE po_table ADD COLUMN POAmount FLOAT AS (ReceivedOuters * ExpectedUnitPricePerOuter)\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RqG1jl2N4rL3"
   },
   "source": [
    "# Creating a blank table for Supplier Invoice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "pQSpX2nE4G0w",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<snowflake.connector.cursor.SnowflakeCursor at 0xffff5b462490>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = {\n",
    "    \"SupplierTransactionID\": \"INTEGER\",\n",
    "    \"SupplierID\": \"INTEGER\",\n",
    "    \"TransactionTypeID\": \"INTEGER\",\n",
    "    \"PurchaseOrderID\": \"INTEGER\",\n",
    "    \"PaymentMethodID\": \"INTEGER\",\n",
    "    \"SupplierInvoiceNumber\": \"INTEGER\",\n",
    "    \"TransactionDate\": \"TEXT\",\n",
    "    \"AmountExcludingTax\": \"REAL\",\n",
    "    \"TaxAmount\": \"REAL\",\n",
    "    \"TransactionAmount\": \"REAL\",\n",
    "    \"OutstandingBalance\": \"REAL\",\n",
    "    \"FinalizationDate\": \"DATE\",\n",
    "    \"IsFinalized\": \"INTEGER\",\n",
    "    \"LastEditedBy\": \"DATE\",\n",
    "    \"LastEditedWhen\": \"TEXT\"\n",
    "}\n",
    "\n",
    "# Step 2: Define the SQL Table Creation Statement\n",
    "create_table_stmt = \"CREATE OR REPLACE TABLE transactions (\" + \", \".join([f\"{col} {dtype}\" for col, dtype in columns.items()]) + \");\"\n",
    "cs.execute(create_table_stmt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iavqWzTN4zLo"
   },
   "source": [
    "# Extracting rows from the XML file and inserting them into the Supplier Invoice table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "lG8Shl5DtzSd",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<snowflake.connector.cursor.SnowflakeCursor at 0xffff5b462490>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1: Parse the XML File\n",
    "tree = ET.parse('Documents/UCSD/495_SQL/Assignment/Group_project/Case_Data/Supplier Transactions XML.xml')\n",
    "root = tree.getroot()\n",
    "\n",
    "all_data = []\n",
    "\n",
    "for row in root.findall('row'):\n",
    "    # Extract column data and handle potential None values\n",
    "    data = [row.find(col).text if row.find(col) is not None else '' for col in columns.keys()]\n",
    "    all_data.append(data)\n",
    "\n",
    "# Construct the INSERT INTO statement\n",
    "placeholders = ', '.join(['%s'] * len(columns))\n",
    "insert_stmt = f\"INSERT INTO transactions VALUES ({placeholders})\"\n",
    "\n",
    "# Step 2: Use executemany to insert all rows\n",
    "cs.executemany(insert_stmt, all_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Mm975Zd5LJv"
   },
   "source": [
    "# Joining Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "ts_DNgW39x1e",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<snowflake.connector.cursor.SnowflakeCursor at 0xffff5b462490>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cs.execute(\"\"\"\n",
    "    CREATE OR REPLACE VIEW joined_data AS\n",
    "    SELECT\n",
    "        pt.PurchaseOrderID AS pt_PurchaseOrderID,\n",
    "        pt.SupplierID AS pt_SupplierID,\n",
    "        pt.OrderDate AS pt_OrderDate,\n",
    "        pt.DeliveryMethodID AS pt_DeliveryMethodID,\n",
    "        pt.ContactPersonID AS pt_ContactPersonID,\n",
    "        pt.ExpectedDeliveryDate AS pt_ExpectedDeliveryDate,\n",
    "        pt.SupplierReference AS pt_SupplierReference,\n",
    "        pt.IsOrderFinalized AS pt_IsOrderFinalized,\n",
    "        pt.Comments AS pt_Comments,\n",
    "        pt.InternalComments AS pt_InternalComments,\n",
    "        pt.LastEditedBy AS pt_LastEditedBy,\n",
    "        pt.LastEditedWhen AS pt_LastEditedWhen,\n",
    "        pt.PurchaseOrderLineID AS pt_PurchaseOrderLineID,\n",
    "        pt.StockItemID AS pt_StockItemID,\n",
    "        pt.OrderedOuters AS pt_OrderedOuters,\n",
    "        pt.Description AS pt_Description,\n",
    "        pt.ReceivedOuters AS pt_ReceivedOuters,\n",
    "        pt.PackageTypeID AS pt_PackageTypeID,\n",
    "        pt.ExpectedUnitPricePerOuter AS pt_ExpectedUnitPricePerOuter,\n",
    "        pt.LastReceiptDate AS pt_LastReceiptDate,\n",
    "        pt.IsOrderLineFinalized AS pt_IsOrderLineFinalized,\n",
    "        pt.Right_LastEditedBy AS pt_Right_LastEditedBy,\n",
    "        pt.Right_LastEditedWhen AS pt_Right_LastEditedWhen,\n",
    "        pt.POAmount AS pt_POAmount,\n",
    "        tr.SupplierTransactionID AS tr_SupplierTransactionID,\n",
    "        tr.SupplierID AS tr_SupplierID,\n",
    "        tr.TransactionTypeID AS tr_TransactionTypeID,\n",
    "        tr.PurchaseOrderID AS tr_PurchaseOrderID,\n",
    "        tr.PaymentMethodID AS tr_PaymentMethodID,\n",
    "        tr.SupplierInvoiceNumber AS tr_SupplierInvoiceNumber,\n",
    "        tr.TransactionDate AS tr_TransactionDate,\n",
    "        tr.AmountExcludingTax AS tr_AmountExcludingTax,\n",
    "        tr.TaxAmount AS tr_TaxAmount,\n",
    "        tr.TransactionAmount AS tr_TransactionAmount,\n",
    "        tr.OutstandingBalance AS tr_OutstandingBalance,\n",
    "        tr.FinalizationDate AS tr_FinalizationDate,\n",
    "        tr.IsFinalized AS tr_IsFinalized,\n",
    "        tr.LastEditedBy AS tr_LastEditedBy,\n",
    "        tr.LastEditedWhen AS tr_LastEditedWhen\n",
    "    FROM po_table pt\n",
    "    JOIN transactions tr USING (PurchaseOrderID)\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XEtXo3u15M2Y"
   },
   "source": [
    "# Calculating the Field for Difference between Amounts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "HlMofqQiAGKY",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<snowflake.connector.cursor.SnowflakeCursor at 0xffff5b462490>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cs.execute(\"\"\"\n",
    "    CREATE OR REPLACE TABLE purchase_orders_and_invoices AS\n",
    "    SELECT *, TR_AmountExcludingTax - PT_POAmount as invoiced_vs_quoted\n",
    "    FROM joined_data\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "3Z4KnMvqCzkV",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<snowflake.connector.cursor.SnowflakeCursor at 0xffff5b462490>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cs.execute(\"\"\"\n",
    "    CREATE MATERIALIZED VIEW purchase_orders_and_invoices1 AS\n",
    "    SELECT *\n",
    "    FROM purchase_orders_and_invoices\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NyhXj70H5QyM"
   },
   "source": [
    "# Extracting Data from Postgres and Loading to Snowflake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 6: Importing supplier_case from postgres to snowflake\n",
    "import psycopg2\n",
    "import csv\n",
    "#Step1: Conntect to Postgres creating supplier_case in VScode\n",
    "postgres_conn = psycopg2.connect(\n",
    "  host = \"127.0.0.1\",\n",
    "  user= \"jovyan\",\n",
    "  port= 8765,\n",
    "  database= \"Northwind\",\n",
    "  password= \"postgres\"\n",
    ")\n",
    "pg_cursor = postgres_conn.cursor()\n",
    "\n",
    "#Step2: Save supplier_case table to a CSV file on a local drive\n",
    "\n",
    "# SQL query to fetch data\n",
    "query = \"SELECT * FROM supplier_case;\"\n",
    "# Execute the query\n",
    "pg_cursor.execute(query)\n",
    "# Fetch all rows\n",
    "rows = pg_cursor.fetchall()\n",
    "# Get column headers\n",
    "column_names = [desc[0] for desc in pg_cursor.description]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Write to CSV file\n",
    "with open(\"Documents/UCSD/495_SQL/Assignment/Group_project/Case_Data/output1.csv\", \"w\", newline=\"\") as f_out:\n",
    "    csv_writer = csv.writer(f_out)\n",
    "    # Write header\n",
    "    csv_writer.writerow(column_names)\n",
    "    # Write data\n",
    "    for row in rows:\n",
    "        csv_writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<snowflake.connector.cursor.SnowflakeCursor at 0xffff5b462490>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#Step3: Upload the CSV file to a Snowflake stage\n",
    "\n",
    "# Create or replace a stage for the CSV file\n",
    "local_csv_path = \"Documents/UCSD/495_SQL/Assignment/Group_project/Case_Data/output1.csv\"\n",
    "stage_name = 'supplier_case_stage'\n",
    "create_stage_sql = f\"CREATE OR REPLACE STAGE {stage_name}\"\n",
    "cs.execute(create_stage_sql)\n",
    "\n",
    "# Upload the CSV file to the stage\n",
    "put_sql = f\"PUT file://{local_csv_path} @{stage_name}/output1.csv\"\n",
    "cs.execute(put_sql)\n",
    "def generate_create_table_sql(csv_filepath):\n",
    "    with open(csv_filepath, 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        headers = next(reader)\n",
    "        first_row = next(reader)\n",
    "\n",
    "    sql = \"CREATE TABLE supplier_case (\\n\"\n",
    "    for header, value in zip(headers, first_row):\n",
    "        if value.isdigit():\n",
    "            datatype = \"INT\"\n",
    "        else:\n",
    "            datatype = \"VARCHAR(255)\"\n",
    "        sql += f\"    {header} {datatype},\\n\"\n",
    "    sql = sql[:-2] + \"\\n);\"\n",
    "    return sql\n",
    "\n",
    "create_table_sql = generate_create_table_sql(local_csv_path)\n",
    "cs.execute(create_table_sql)\n",
    "\n",
    "copy_into_sql = f\"\"\"\n",
    "    COPY INTO supplier_case\n",
    "    FROM @{stage_name}/output1.csv\n",
    "    FILE_FORMAT = (TYPE = 'CSV' SKIP_HEADER = 1 FIELD_OPTIONALLY_ENCLOSED_BY = '\"' NULL_IF=('NULL', '\\\\N'));\n",
    "\"\"\"\n",
    "cs.execute(copy_into_sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mXZd46bI5aex"
   },
   "source": [
    "# Connect to Snowflake Marketplace and Extract Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting geopandas\n",
      "  Downloading geopandas-0.13.2-py3-none-any.whl (1.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m0m\n",
      "\u001b[?25hCollecting fiona>=1.8.19 (from geopandas)\n",
      "  Downloading Fiona-1.9.4.post1.tar.gz (924 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m924.3/924.3 kB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: packaging in /opt/conda/lib/python3.11/site-packages (from geopandas) (23.1)\n",
      "Requirement already satisfied: pandas>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from geopandas) (2.0.2)\n",
      "Collecting pyproj>=3.0.1 (from geopandas)\n",
      "  Downloading pyproj-3.6.0-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (6.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.1/6.1 MB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting shapely>=1.7.1 (from geopandas)\n",
      "  Downloading shapely-2.0.1-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (2.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: attrs>=19.2.0 in /opt/conda/lib/python3.11/site-packages (from fiona>=1.8.19->geopandas) (23.1.0)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.11/site-packages (from fiona>=1.8.19->geopandas) (2023.5.7)\n",
      "Requirement already satisfied: click~=8.0 in /opt/conda/lib/python3.11/site-packages (from fiona>=1.8.19->geopandas) (8.1.3)\n",
      "Collecting click-plugins>=1.0 (from fiona>=1.8.19->geopandas)\n",
      "  Downloading click_plugins-1.1.1-py2.py3-none-any.whl (7.5 kB)\n",
      "Collecting cligj>=0.5 (from fiona>=1.8.19->geopandas)\n",
      "  Downloading cligj-0.7.2-py3-none-any.whl (7.1 kB)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.11/site-packages (from fiona>=1.8.19->geopandas) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas>=1.1.0->geopandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas>=1.1.0->geopandas) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.11/site-packages (from pandas>=1.1.0->geopandas) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /opt/conda/lib/python3.11/site-packages (from pandas>=1.1.0->geopandas) (1.23.5)\n",
      "Building wheels for collected packages: fiona\n",
      "  Building wheel for fiona (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for fiona: filename=Fiona-1.9.4.post1-cp311-cp311-linux_aarch64.whl size=815028 sha256=e245b7b066928d9d6879c06149129d9afe568748c047b820a29dc34988ec7c2a\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/f2/95/39/af34052698e2fe83590532f3710066530eb1278effd07bca74\n",
      "Successfully built fiona\n",
      "Installing collected packages: shapely, pyproj, cligj, click-plugins, fiona, geopandas\n",
      "Successfully installed click-plugins-1.1.1 cligj-0.7.2 fiona-1.9.4.post1 geopandas-0.13.2 pyproj-3.6.0 shapely-2.0.1\n"
     ]
    }
   ],
   "source": [
    "# Q7: Part 0: Installing Geopandas to turn shape file into csv\n",
    "!pip install geopandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q7: Part 1: Snowflake Marketplace to the Knoema Environment Data Atlas\n",
    "import geopandas as gpd\n",
    "\n",
    "# Read shapefiles for geodata mapping\n",
    "gdf = gpd.read_file('Documents/UCSD/495_SQL/Assignment/Group_project/Case_Data/tl_2020_us_zcta520/tl_2020_us_zcta520.shp')\n",
    "\n",
    "# Moving Data to Snowflake\n",
    "gdf['geometry'] = gdf['geometry'].apply(lambda x: x.wkt)\n",
    "\n",
    "\n",
    "# Save to CSV\n",
    "gdf.to_csv('Documents/UCSD/495_SQL/Assignment/Group_project/Case_Data/tl_2020_us_zcta520/tl_2020_us_zcta520.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q7 Part 2\n",
    "# Create Table in Snowflake\n",
    "cs.execute(\n",
    "\"\"\"\n",
    "CREATE OR REPLACE TABLE   \n",
    "ZC_GeoLocation (\n",
    "            ZCTA5CE20 INTEGER,\n",
    "            GEOID20 INTEGER,\n",
    "            CLASSFP20 TEXT,\n",
    "            MTFCC20 TEXT,\n",
    "            FUNCSTAT20 TEXT,\n",
    "            ALAND20 TEXT,\n",
    "            AWATER20 TEXT,\n",
    "            INTPTLAT20 NUMERIC,\n",
    "            INTPTLON20 NUMERIC,\n",
    "            geometry TEXT\n",
    "                )\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "csv_path = 'Documents/UCSD/495_SQL/Assignment/Group_project/Case_Data/tl_2020_us_zcta520/tl_2020_us_zcta520.csv'\n",
    "\n",
    "# staging the data in the xml file\n",
    "put_command = f\"PUT file://{csv_path} @%ZC_GeoLocation\"\n",
    "cs.execute(put_command)\n",
    "print(put_command)\n",
    "# copying staged supplier data into new snowflake table\n",
    "copy_command = \"\"\"\n",
    "COPY INTO ZC_GeoLocation (ZCTA5CE20, GEOID20, CLASSFP20, MTFCC20, FUNCSTAT20, ALAND20, AWATER20, INTPTLAT20, INTPTLON20, geometry)\n",
    "FROM @%ZC_GeoLocation\n",
    "FILE_FORMAT = (TYPE = 'CSV' FIELD_OPTIONALLY_ENCLOSED_BY = '\"' SKIP_HEADER = 1)\n",
    "\"\"\"\n",
    "\n",
    "cs.execute(copy_command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q7 Part 3 - JOINING GEOLOCATION DATA TO SUPPLIER_CASE TABLE (DISTINCT ZIPS)\n",
    "cs.execute(\n",
    "    \"\"\"\n",
    "    CREATE OR REPLACE TABLE SUPPLIERZIP_TO_GEOLOCATION AS\n",
    "        SELECT Distinct S.postalpostalcode, z.INTPTLAT20, z.INTPTLON20\n",
    "        FROM supplier_case s\n",
    "        JOIN ZC_GeoLocation z on z.GEOID20 = s.postalpostalcode\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "rows = cs.fetchall()\n",
    "for row in rows:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q7 Part 4: CROSS JOIN and RANK TO CREATE TABLE WITH ZIP AND CLOSEST WEATHER STATION\n",
    "\n",
    "cs.execute(\n",
    "\"\"\"\n",
    "CREATE OR REPLACE TABLE CLOSEST_STATIONS AS\n",
    "SELECT * FROM (\n",
    "SELECT DISTINCT SZ.POSTALPOSTALCODE, EN.STATIONSNAME,\n",
    "RANK() over(partition by postalpostalcode order by st_distance(st_makepoint(SZ.INTPTLON20,SZ.INTPTLAT20),st_makepoint(EN.STATIONSLONGITUDE,EN.STATIONSLATITUDE)) asc) as rank,\n",
    "st_distance(st_makepoint(SZ.INTPTLON20,SZ.INTPTLAT20),st_makepoint(EN.STATIONSLONGITUDE,EN.STATIONSLATITUDE)) as min_distance\n",
    "FROM SUPPLIERZIP_TO_GEOLOCATION SZ, DISTINCT_NOAACD2019R EN\n",
    ")\n",
    "WHERE rank = 1\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "rows = cs.fetchall()\n",
    "for row in rows:\n",
    "    print(row)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q7: Part 5: Joining above table of closest stations to get all daily weather data\n",
    "cs.execute(\n",
    "    \"\"\"\n",
    "    CREATE OR REPLACE TABLE supplier_zip_code_weather as\n",
    "        select \n",
    "            cs.postalpostalcode as zip_code,\n",
    "            n.\"Date\" as date,\n",
    "            n.\"Value\" as daily_high_temp\n",
    "        from CLOSEST_STATIONS cs\n",
    "        join NOAACD2019R n on n.stationsname = cs.stationsname\n",
    "        where \"Indicator Name\" = 'Maximum temperature (Fahrenheit)'\n",
    "        order by postalpostalcode, date\n",
    "    \"\"\"\n",
    ")\n",
    "rows = cs.fetchall()\n",
    "for row in rows:\n",
    "    print(row)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ES44L5zv5eWq"
   },
   "source": [
    "# Final Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q8 Join purchase_orders_and_invoices, supplier_case, and supplier_zip_code_weather based on zip codes and the transaction date\n",
    "cs.execute(\n",
    "    \"\"\"\n",
    "CREATE OR REPLACE TABLE FINAL_PO_WEATHER AS\n",
    "    SELECT pi.*, sw.daily_high_temp as daily_high_temp\n",
    "    FROM purchase_orders_and_invoices pi\n",
    "    JOIN supplier_case sc on sc.supplierid = pi.supplierid\n",
    "    JOIN supplier_zip_code_weather sw on sw.zip_code = postalpostalcode and sw.date = transactiondate\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "rows = cs.fetchall()\n",
    "for row in rows:\n",
    "    print(row)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
